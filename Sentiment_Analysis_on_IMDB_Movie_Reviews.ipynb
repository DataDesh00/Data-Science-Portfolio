{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis on IMDB Movie Reviews.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlYc8DmLEYUO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "0f3ae0d5-b54c-4535-c8df-6eda41bb1c91"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.5.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_265\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n",
            "Collecting pyspark==2.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 58kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 42.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130389 sha256=20923c797d2c6e410c328b695fce3c8ce6ffaa1b3e759afbb542d488e8ad9b58\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp==2.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/b4/db653f8080a446de8ce981b262d85c85c61de7e920930726da0d1c6b4c65/spark_nlp-2.5.1-py2.py3-none-any.whl (121kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 4.4MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tcDtAHxEdvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "b10b3184-3a4b-4964-bfc0-b2a390d1b877"
      },
      "source": [
        "! wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-14 21:06:30--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  37.6MB/s    in 2.1s    \n",
            "\n",
            "2020-09-14 21:06:33 (37.6 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kJaM-MEEgFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! tar xzf aclImdb_v1.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSsZIgSkFfHA",
        "colab_type": "text"
      },
      "source": [
        "Sentiment analysis ins one of the most popular uses of NLP that allows to leverage the data from websites with comments and ratings to learn the relationship betwee the language used in positive or negative sentiment.\n",
        "\n",
        "The objective of this study is to build a model that evaluates movie reviews. Many movie reviewers use some quantifiable metrics such as thumbs up/down or stars. However, there are some reviewers who might use different metrics (a 10 point scale) or no metrics at all. It might be bettere if a model is built to look at the reviews and produce a score based on the text of the review instead of on an ad-hoc score.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qISbWjaFEklX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary libraries\n",
        "\n",
        "import sparknlp\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp import DocumentAssembler, Finisher\n",
        "from sparknlp.annotator import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dil8bl1xIFXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = sparknlp.start()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjwh4VHdIOlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_train = spark.sparkContext.wholeTextFiles('aclImdb/train/pos/')\n",
        "neg_train = spark.sparkContext.wholeTextFiles('aclImdb/train/neg')\n",
        "pos_test = spark.sparkContext.wholeTextFiles('aclImdb/test/pos')\n",
        "neg_test = spark.sparkContext.wholeTextFiles('aclImdb/test/neg')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYHvxbreJAb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_train = spark.createDataFrame(pos_train, ['path', 'text'])\n",
        "pos_train = pos_train.repartition(100)\n",
        "pos_train = pos_train.withColumn('label', lit(1)).persist()\n",
        "neg_train = spark.createDataFrame(neg_train, ['path', 'text'])\n",
        "neg_train = neg_train.repartition(100)\n",
        "neg_train = neg_train.withColumn('label', lit(0)).persist()\n",
        "pos_test = spark.createDataFrame(pos_test, ['path', 'text'])\n",
        "pos_test = pos_test.repartition(100)\n",
        "pos_test = pos_test.withColumn('label', lit(1)).persist()\n",
        "neg_test = spark.createDataFrame(neg_test, ['path', 'text'])\n",
        "neg_test = neg_test.repartition(100)\n",
        "neg_test = neg_test.withColumn('label', lit(0)).persist()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGHmNEaQJTXw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "fd58277c-57fd-4b70-eeee-cddfb4622dce"
      },
      "source": [
        "print(pos_train.first()['text'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finally got to see this classic TV movie on an unofficial disc recorded from an old VHS, it is a classic piece of horror. Its a pity more of this neglected corner of horror in terms of official releases on DVD and VHS ... the TV horror movie. Recommended for all fans of the 70's TV movie much like trilogy of terror. Those interested should get the book on the subject by David Deal - Television Fright Films of the 70's. Email me for a chance to see it.....its fabulous to see it again.<br /><br />It does have it problems like many TV movies they have to be rather inventive in the effects dept and even at 70 mins it can seem to drag possibly we are all used to more modern editing but still great stuff and far better than many theatrical frights released today.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8BLXaabP2ZE",
        "colab_type": "text"
      },
      "source": [
        "This seems like a clearly positive review. It is possible to identify a few words that seem like a good signal, like 'enjoyed'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuFZOMPSPzQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f8666be1-5430-40e8-9695-459885a03618"
      },
      "source": [
        "print(neg_train.first()['text'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lorne Michaels once again proves that he has absolutely no business producing movies.<br /><br />You'd think that after such dismal flicks \"Superstar\", \"Night at the Roxbury\", and \"Coneheads\", he'd start to get the notion that maybe he doesn't know what he's doing when it comes to movies (and many would argue that he doesn't know what he's doing when it comes to television, as well). Trying to make feature films out of skits that wore out their welcome the third time the were done on SNL makes no sense.<br /><br />I personally like Tim Meadows, and think that he would be great in the right movie. It's a shame to see a talented guy wasted in a film that features unfunny after unfunny situation, and caps it all with a dreadfully bad song and dance scene. Any laughs here will be because the movie is so bad, not because it's funny.<br /><br />Oh well, at least we can be thankful that there are many other tired SNL characters who will never have films done about them. It's just too bad that this one made it to the big screen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_eT0QseRH7N",
        "colab_type": "text"
      },
      "source": [
        "Here is an example of a negative review.\n",
        "\n",
        "There are also some HTML tags that need to be removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmQN7GKUJbkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2ca8a229-10ef-4d9a-f50f-9371b15cf6b8"
      },
      "source": [
        "# corpus size\n",
        "print('pos_train size', pos_train.count())\n",
        "print('neg_train size', neg_train.count())\n",
        "print('pos_test size', pos_test.count())\n",
        "print('neg_test size', neg_test.count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_train size 12500\n",
            "neg_train size 12500\n",
            "pos_test size 12500\n",
            "neg_test size 12500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDYkrDK5SBAV",
        "colab_type": "text"
      },
      "source": [
        "So there are 50000 documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyHmlkQlR66O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "f7656117-6481-4a05-c85e-7c51b406f3ae"
      },
      "source": [
        "# length of the text\n",
        "pos_train.selectExpr('length(text) AS text_len').toPandas().describe()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>12500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1347.160240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1046.747365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>70.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>695.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>982.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1651.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>13704.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           text_len\n",
              "count  12500.000000\n",
              "mean    1347.160240\n",
              "std     1046.747365\n",
              "min       70.000000\n",
              "25%      695.000000\n",
              "50%      982.000000\n",
              "75%     1651.000000\n",
              "max    13704.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS0b1T0zS-j_",
        "colab_type": "text"
      },
      "source": [
        "There is a lot of variation in character lenghts.\n",
        "\n",
        "Text length may seem very low level. \n",
        "\n",
        "Longer comments may be more likely to be negative.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlXfVNniTvrz",
        "colab_type": "text"
      },
      "source": [
        "# **Building the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwnReDMMS8zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining postives and negatives into two data sets: train and test\n",
        "train = pos_train.unionAll(neg_train)\n",
        "test = pos_test.unionAll(neg_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oa7A3ufSLxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "06bd6e7d-9a36-429e-f878-f3e4686dafda"
      },
      "source": [
        "# creating pipeline\n",
        "assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "sentence = SentenceDetector().setInputCols('document').setOutputCol('sentences')\n",
        "tokenizer = Tokenizer().setInputCols('sentences').setOutputCol('tokens')\n",
        "lemmatizer = LemmatizerModel.pretrained().setInputCols('tokens').setOutputCol('lemmas')\n",
        "normalizer = Normalizer().setCleanupPatterns(['[^a-zA-Z.-]+', '^[^a-zA-Z]+', '[^a-zA-Z]+$',])\\\n",
        "             .setInputCols('lemmas').setOutputCol('normalized').setLowercase(True)\n",
        "\n",
        "glove = WordEmbeddingsModel.pretrained(name='glove_100d').setInputCols(['document', 'normalized'])\\\n",
        "             .setOutputCol('embeddings')\n",
        "\n",
        "nlp_pipeline = Pipeline().setStages([assembler, sentence, tokenizer, lemmatizer,\n",
        "                                     normalizer, glove]).fit(train)\n",
        "\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTlJd9EBUZGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# selecting the values, original data, normalized tokens and embeddings\n",
        "train = nlp_pipeline.transform(train).selectExpr('path', 'text', 'label',\n",
        "                                                 'normalized.result AS normalized',\n",
        "                                                 'embeddings.embeddings')\n",
        "\n",
        "test = nlp_pipeline.transform(test).selectExpr('path', 'text', 'label',\n",
        "                                               'normalized.result AS normalized',\n",
        "                                               'embeddings.embeddings')\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoWiO5GAeGNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_pipeline.write().overwrite().save('nlp_pipeline_1')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmJZbs8ViBmV",
        "colab_type": "text"
      },
      "source": [
        "Creating a simpe version of doc2vec, computing the average of teh word vectors in a document to create a document vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED-gCn9ViNnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
        "\n",
        "def avg_wordvecs_fun(wordvecs):\n",
        "  return DenseVector(np.mean(wordvecs, axis=0))\n",
        "\n",
        "avg_wordvecs = spark.udf.register('avg_wordvecs', avg_wordvecs_fun, returnType=VectorUDT())\n",
        "\n",
        "train = train.withColumn('avg_wordvec', avg_wordvecs('embeddings'))\n",
        "test = test.withColumn('avg_wordvec', avg_wordvecs('embeddings'))\n",
        "train = train.drop('embeddings')\n",
        "test = test.drop('ebeddings')\n",
        "train = train.persist()\n",
        "test = test.persist()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rw697GzjQiA",
        "colab_type": "text"
      },
      "source": [
        "Featurizing with TF.IDF features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LGAUPSNjJUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer, IDF"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7qqhBjmjaXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf = CountVectorizer().setInputCol('normalized').setOutputCol('tf')\n",
        "idf = IDF().setInputCol('tf').setOutputCol('tdifd')\n",
        "\n",
        "featurizer = Pipeline().setStages([tf, idf])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43AZl3PkjyNd",
        "colab_type": "text"
      },
      "source": [
        "Building the model using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZwyOvDpjtoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1ezKeXej_v3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_assembler = VectorAssembler().setInputCols(['avg_wordvec']).setOutputCol('features')\n",
        "log_reg = LogisticRegression().setFeaturesCol('features').setLabelCol('label')\n",
        "\n",
        "model_pipeline = Pipeline().setStages([featurizer, vec_assembler, log_reg])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfFGRA_2kdm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model_pipeline.fit(train)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt-1iL7wulsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_preds = model.transform(train)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozF-ie70urYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_preds = model.transform(test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1USzUj0lZIh",
        "colab_type": "text"
      },
      "source": [
        "# **Model Evaluation**\n",
        "The model is evaluated computing F1 score on train and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31Engz_zki1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s9E-nkylo2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator().setMetricName('f1')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxkV9XyKlulF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "40a443fa-af86-4b33-9c4d-a76c2e2f8cef"
      },
      "source": [
        "evaluator.evaluate(train_preds)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8025998329204984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5aTiYlXlyB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5737574f-24f6-4f1e-f878-4dd373bb36bf"
      },
      "source": [
        "evaluator.evaluate(test_preds)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8015517696374732"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}